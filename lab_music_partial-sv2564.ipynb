{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-65EDVHcZNTW"
   },
   "source": [
    "# Lab:  Neural Networks for Music Classification\n",
    "\n",
    "In addition to the concepts in the [MNIST neural network demo](./demo2_mnist_neural.ipynb), in this lab, you will learn to:\n",
    "* Load a file from a URL\n",
    "* Extract simple features from audio samples for machine learning tasks such as speech recognition and classification\n",
    "* Build a simple neural network for music classification using these features\n",
    "* Use a callback to store the loss and accuracy history in the training process\n",
    "* Optimize the learning rate of the neural network\n",
    "\n",
    "To illustrate the basic concepts, we will look at a relatively simple music classification problem.  Given a sample of music, we want to determine which instrument (e.g. trumpet, violin, piano) is playing.  This dataset was generously supplied by [Prof. Juan Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) at NYU Stenihardt  and his former PhD student Eric Humphrey (now at Spotify).  They have a complete website dedicated to deep learning methods in music informatics:\n",
    "\n",
    "http://marl.smusic.nyu.edu/wordpress/projects/feature-learning-deep-architectures/deep-learning-python-tutorial/\n",
    "\n",
    "You can also check out Juan's <a href=\"http://www.nyu.edu/classes/bello/ACA.html\">course</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDl9u5MOZNTX"
   },
   "source": [
    "## Loading Tensorflow\n",
    "\n",
    "Before starting this lab, you will need to install [Tensorflow](https://www.tensorflow.org/install/).  If you are using [Google colaboratory](https://colab.research.google.com), Tensorflow is already installed.  Run the following command to ensure Tensorflow is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlhkoceAZNTX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7aHuM7LZNTX"
   },
   "source": [
    "Then, load the other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIEUcIX6ZNTY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2EQvdkyZNTY"
   },
   "source": [
    "## Audio Feature Extraction with Librosa\n",
    "\n",
    "The key to audio classification is to extract the correct features. In addition to `keras`, we will need the `librosa` package.  The `librosa` package in python has a rich set of methods extracting the features of audio samples commonly used in machine learning tasks such as speech recognition and sound classification. \n",
    "\n",
    "Installation instructions and complete documentation for the package are given on the [librosa main page](https://librosa.github.io/librosa/).  On most systems, you should be able to simply use:\n",
    "\n",
    "    pip install librosa\n",
    "    \n",
    "For Unix, you may need to load some additional packages:\n",
    "\n",
    "    sudo apt-get install build-essential\n",
    "    sudo apt-get install libxext-dev python-qt4 qt4-dev-tools\n",
    "    pip install librosa\n",
    "    \n",
    "After you have installed the package, try to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v15M0pGxZNTY"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS366vFyZNTY"
   },
   "source": [
    "In this lab, we will use a set of music samples from the website:\n",
    "\n",
    "http://theremin.music.uiowa.edu\n",
    "\n",
    "This website has a great set of samples for audio processing.  Look on the web for how to use the `requests.get` and `file.write` commands to load the file at the URL provided into your working directory.\n",
    "\n",
    "You can play the audio sample by copying the file to your local machine and playing it on any media player.  If you listen to it you will hear a soprano saxaphone (with vibrato) playing four notes (C, C#, D, Eb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IIPRqnlZNTZ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "fn = \"SopSax.Vib.pp.C6Eb6.aiff\"\n",
    "url = \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/\"+fn\n",
    "\n",
    "# TODO 1:  Load the file from url and save it in a file under the name fn\n",
    "with open (fn, \"wb\") as f:\n",
    "  f.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMVBLf0KZNTZ"
   },
   "source": [
    "Next, use `librosa` command `librosa.load` to read the audio file with filename `fn` and get the samples `y` and sample rate `sr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6kMKAC3ZNTZ"
   },
   "outputs": [],
   "source": [
    "# TODO 2\n",
    "y, sr = librosa.load(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5ClOaYYZNTZ"
   },
   "source": [
    "Extracting features from audio files is an entire subject on its own right.  A commonly used set of features are called the Mel Frequency Cepstral Coefficients (MFCCs).  These are derived from the so-called mel spectrogram which is something like a regular spectrogram, but the power and frequency are represented in log scale, which more naturally aligns with human perceptual processing.  You can run the code below to display the mel spectrogram from the audio sample.\n",
    "\n",
    "You can easily see the four notes played in the audio track.  You also see the 'harmonics' of each notes, which are other tones at integer multiples of the fundamental frequency of each note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "e5FzpYxGZNTZ",
    "outputId": "9dc2e661-f920-4f9d-d853-76f8c7807ffb"
   },
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
    "                         y_axis='mel', fmax=8000, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E_Cr8weZNTZ"
   },
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Using the MFCC features described above, Eric Humphrey and Juan Bellow have created a complete data set that can used for instrument classification.  Essentially, they collected a number of data files from the website above.  For each audio file, the segmented the track into notes and then extracted 120 MFCCs for each note.  The goal is to recognize the instrument from the 120 MFCCs.  The process of feature extraction is quite involved.  So, we will just use their processed data provided at:\n",
    "\n",
    "https://github.com/marl/dl4mir-tutorial/blob/master/README.md\n",
    "\n",
    "Note the password.  Load the four files into some directory, say  `instrument_dataset`.  Then, load them with the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-AVtDvWigTf",
    "outputId": "9b5b72bc-0613-419c-aa1c-66baff74b4ec"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkgL2m62ZNTZ"
   },
   "outputs": [],
   "source": [
    "data_dir = '/content/drive/MyDrive/Colab_Notebooks/instrument_dataset'\n",
    "Xtr = np.load(data_dir+'/uiowa_train_data.npy')\n",
    "ytr = np.load(data_dir+'/uiowa_train_labels.npy')\n",
    "Xts = np.load(data_dir+'/uiowa_test_data.npy')\n",
    "yts = np.load(data_dir+'/uiowa_test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BFgBkoMZNTa"
   },
   "source": [
    "Looking at the data files:\n",
    "* What are the number of training and test samples?\n",
    "* What is the number of features for each sample?\n",
    "* How many classes (i.e. instruments) are there per class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joPEy_JLZNTa",
    "outputId": "8d537366-673d-4285-ae48-7b4ba770d7ac"
   },
   "outputs": [],
   "source": [
    "# TODO 3\n",
    "print('training samples number: ',Xtr.shape[0])\n",
    "print('features per sample: ',Xtr.shape[1])\n",
    "print('test samples number:  ',Xts.shape[0])\n",
    "print('classes per class: ',np.unique(ytr).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qgiUxoOZNTa"
   },
   "source": [
    "Before continuing, you must scale the training and test data, `Xtr` and `Xts`.  Compute the mean and std deviation of each feature in `Xtr` and create a new training data set, `Xtr_scale`, by subtracting the mean and dividing by the std deviation.  Also compute a scaled test data set, `Xts_scale` using the mean and std deviation learned from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKJjnnmeZNTa"
   },
   "outputs": [],
   "source": [
    "# TODO 4: Scale the training and test matrices\n",
    "mean = np.mean(Xtr, axis=0)\n",
    "st_deviation = np.std(Xtr, axis=0)\n",
    "\n",
    "Xtr_scale = (Xtr - mean[None, :])/st_deviation[None, :]\n",
    "Xts_scale = (Xts - mean[None, :])/st_deviation[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "7WCF-sA9ZNTa"
   },
   "source": [
    "## Building a Neural Network Classifier\n",
    "\n",
    "Following the example in [MNIST neural network demo](./mnist_neural.ipynb), clear the keras session.  Then, create a neural network `model` with:\n",
    "* `nh=256` hidden units\n",
    "* `sigmoid` activation\n",
    "* select the input and output shapes correctly\n",
    "* print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb8DuGCSZNTa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn3uu2PdZNTa"
   },
   "outputs": [],
   "source": [
    "# TODO 5 clear session\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwbLUA1lZNTa"
   },
   "outputs": [],
   "source": [
    "# TODO 6: construct the model\n",
    "nin = Xtr.shape[1]  # dimension of input data\n",
    "nh = 256     # number of hidden units\n",
    "nout = int(np.max(ytr)+1)    \n",
    "model = Sequential()\n",
    "model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(units=nout, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfIsEV4tZNTa",
    "outputId": "ec62a2e1-738f-4277-e71a-c9a0bff431c9"
   },
   "outputs": [],
   "source": [
    "# TODO 7:  Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xdfiI9SZNTa"
   },
   "source": [
    "Create an optimizer and compile the model.  Select the appropriate loss function and metrics.  For the optimizer, use the Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRxn6n-AZNTb",
    "outputId": "923bd8a9-5486-48ab-87f0-d6279dfdae20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 8\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001) \n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuVQHbb-ZNTb"
   },
   "source": [
    "Fit the model for 10 epochs using the scaled data for both the training and validation.  Use the `validation_data` option to pass the test data.  Also, pass the callback class create above.  Use a batch size of 100.  Your final accuracy should be >99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5bklZv4ZNTb",
    "outputId": "e024b5c4-474e-45c3-e861-764f752ba8ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 9\n",
    "hist = model.fit(Xtr_scale, ytr, epochs=10, batch_size=100, validation_data=(Xts_scale,yts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjD9lG7XZNTb"
   },
   "source": [
    "Plot the validation accuracy saved in `hist.history` dictionary. This gives one accuracy value per epoch.  You should see that the validation accuracy saturates at a little higher than 99%.  After that it \"bounces around\" due to the noise in the stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "JOuLMLxxZNTb",
    "outputId": "241ad695-f32e-437e-d856-2ef692062d98"
   },
   "outputs": [],
   "source": [
    "# TODO 10\n",
    "tr_accuracy = hist.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy']\n",
    "\n",
    "plt.plot(tr_accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuarcy')\n",
    "plt.legend(['training accuracy', 'validation accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVNoRD0QZNTb"
   },
   "source": [
    "Plot the loss values saved in the `hist.history` dictionary.  You should see that the loss is steadily decreasing.  Use the `semilogy` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "xolD8mWCZNTb",
    "outputId": "4caf1baa-4522-46e3-f331-afe5fb1388b0"
   },
   "outputs": [],
   "source": [
    "# TODO 11\n",
    "plt.semilogy(np.array(hist.history['loss']))\n",
    "plt.ylabel('loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dukislNhZNTb"
   },
   "source": [
    "## Optimizing the Learning Rate\n",
    "\n",
    "One challenge in training neural networks is the selection of the learning rate.  Rerun the above code, trying four learning rates as shown in the vector `rates`.  For each learning rate:\n",
    "* clear the session\n",
    "* construct the network\n",
    "* select the optimizer.  Use the Adam optimizer with the appropriate learrning rate.\n",
    "* train the model for 20 epochs\n",
    "* save the accuracy and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kVRhlTaZNTb",
    "outputId": "46778918-5aa0-46aa-e9c0-417dc7ffc16d"
   },
   "outputs": [],
   "source": [
    "rates = [0.01,0.001,0.0001]\n",
    "batch_size = 100\n",
    "loss_hist = []\n",
    "accuracy_list = []\n",
    "\n",
    "# TODO 12\n",
    "for lr in rates:\n",
    "\n",
    "  K.clear_session()\n",
    "\n",
    "  nin = Xtr.shape[1]  # dimension of input data\n",
    "  nh = 256     # number of hidden units\n",
    "  nout = int(np.max(ytr)+1)  \n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "  model.add(Dense(units=nout, activation='softmax', name='output'))\n",
    "\n",
    "  opt = optimizers.Adam(lr=lr) \n",
    "  model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  hist = model.fit(Xtr_scale, ytr, epochs=20, batch_size=100, validation_data=(Xts_scale,yts))\n",
    "\n",
    "  loss_hist.append(hist.history['loss'])\n",
    "  accuracy_list.append(hist.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUtuyW7IZNTb"
   },
   "source": [
    "Plot the loss funciton vs. the epoch number for all three learning rates on one graph.  You should see that the lower learning rates are more stable, but converge slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "S3Ly-D5qZNTb",
    "outputId": "d92f2106-1a6d-43e3-f3bc-b404be87c3f6"
   },
   "outputs": [],
   "source": [
    "# TODO 13\n",
    "\n",
    "plt.plot(np.arange(1,21), np.array(loss_hist)[0, :], label='0.01')\n",
    "plt.plot(np.arange(1,21), np.array(loss_hist)[1, :], label='0.001')\n",
    "plt.plot(np.arange(1,21), np.array(loss_hist)[2, :], label='0.0001')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77eYcJPHZNTc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJOCu6T8ZNTc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
